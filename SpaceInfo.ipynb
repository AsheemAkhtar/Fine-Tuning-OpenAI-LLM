{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Installing Required Libraries"
      ],
      "metadata": {
        "id": "xT_6cHKkpsvL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tb_OYIlXpkLL"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Relevant Libraries"
      ],
      "metadata": {
        "id": "8BQPaAXtpzrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "import json\n",
        "import time"
      ],
      "metadata": {
        "id": "y30_phBqpw98"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=\"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\")"
      ],
      "metadata": {
        "id": "7CBXCZjKqT3E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting Drive"
      ],
      "metadata": {
        "id": "oeKI4zf9qlN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/fine/space.txt'  # Adjust if needed\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    fiction_text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM1pX32Uqg78",
        "outputId": "f88b6823-7c4f-4254-c2ac-c1a09d3061b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Data in JSONL Format"
      ],
      "metadata": {
        "id": "NyHUusRqrdTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split text into logical story segments\n",
        "chunks = fiction_text.split(\"\\n\\n\")\n",
        "\n",
        "# Prepare JSONL format\n",
        "fine_tune_data = []\n",
        "for chunk in chunks:\n",
        "    fine_tune_data.append({\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a creative fiction writer.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Continue this story: \" + chunk},\n",
        "            {\"role\": \"assistant\", \"content\": \"...\" }  # Expected response placeholder\n",
        "        ]\n",
        "    })\n",
        "\n",
        "# Save JSONL file\n",
        "jsonl_path = \"/content/fiction_finetune.jsonl\"\n",
        "with open(jsonl_path, 'w') as f:\n",
        "    for entry in fine_tune_data:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"Formatted data saved at: {jsonl_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ9r2mUkq2Zc",
        "outputId": "09c8ae6d-6028-4fe6-f7ce-2783428994a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatted data saved at: /content/fiction_finetune.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading JSONL File to OpenAI"
      ],
      "metadata": {
        "id": "rxJfC1rbtQqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_response = client.files.create(\n",
        "    file=open(jsonl_path, \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "file_id = file_response.id\n",
        "print(\"File uploaded successfully. File ID:\", file_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UfP7teErlh3",
        "outputId": "083ad012-8879-441b-9a07-062750fd8e98"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File uploaded successfully. File ID: file-AaHfRc1kmVJjYcFEGzPZZE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Starting Fine-Tuning"
      ],
      "metadata": {
        "id": "pqb0c_rltfDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_response = client.fine_tuning.jobs.create(\n",
        "    training_file=file_id,\n",
        "    model=\"gpt-4o-mini-2024-07-18\"\n",
        ")\n",
        "\n",
        "fine_tune_job_id = fine_tune_response.id\n",
        "print(\"Fine-tuning started. Job ID:\", fine_tune_job_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMfSINontVjB",
        "outputId": "42507676-0680-4991-812e-1d114ec4a365"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning started. Job ID: ftjob-vS6vBIstBtEtoGfIPidjmYv4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOnitoring"
      ],
      "metadata": {
        "id": "WWe9PQrDtuPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    job_status = client.fine_tuning.jobs.retrieve(fine_tune_job_id)\n",
        "    print(\"Fine-tuning status:\", job_status.status)\n",
        "\n",
        "    if job_status.status == \"succeeded\":\n",
        "        print(\"Fine-tuning completed! Model ID:\", job_status.fine_tuned_model)\n",
        "        break\n",
        "    elif job_status.status == \"failed\":\n",
        "        print(\"Fine-tuning failed. Check OpenAI logs.\")\n",
        "        break\n",
        "\n",
        "    time.sleep(60)  # Wait 1 minute before checking again"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlY3JASitmXD",
        "outputId": "e5b5a0d2-e025-454e-9126-c4176a5d2a34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning status: validating_files\n",
            "Fine-tuning status: running\n",
            "Fine-tuning status: running\n",
            "Fine-tuning status: running\n",
            "Fine-tuning status: running\n",
            "Fine-tuning status: succeeded\n",
            "Fine-tuning completed! Model ID: ft:gpt-4o-mini-2024-07-18:personal::B9Qs93MK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Fine-Tuned Model"
      ],
      "metadata": {
        "id": "M3rT1K4EvRsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt user for a question\n",
        "user_input = input(\"üìù Type your question: \")\n",
        "\n",
        "# Send the question to OpenAI\n",
        "response = client.chat.completions.create(\n",
        "    model=\"ft:gpt-4o-mini-2024-07-18:personal::B9Qs93MK\",\n",
        "    messages=[{\"role\": \"user\", \"content\": user_input}]\n",
        ")\n",
        "\n",
        "# Print AI's response\n",
        "print(\"\\nü§ñ AI Response:\\n\", response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpBV219rtwoJ",
        "outputId": "5159e7bf-8515-4973-c114-dc69d08d2c91"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Type your question: does dark matter decay?\n",
            "\n",
            "ü§ñ AI Response:\n",
            " As of my last knowledge update in October 2023, the nature of dark matter remains one of the unsolved problems in physics. There are several theories regarding dark matter, but it is generally expected not to decay in the conventional sense like ordinary matter or certain unstable particles. \n",
            "\n",
            "Dark matter is hypothesized to be composed of non-baryonic particles, which may include WIMPs (Weakly Interacting Massive Particles), axions, or sterile neutrinos, among other candidates. Many of these candidate particles are stable over cosmological timescales, which means they do not undergo decay processes like those found in ordinary matter.\n",
            "\n",
            "However, some theories suggest the possibility of interactions that could lead to certain decay processes, particularly under specific conditions or through interactions with other fields or particles. Research in particle physics and cosmology continues to explore these possibilities, including potential indirect indications that would imply decay or transformation processes.\n",
            "\n",
            "In conclusion, while the current understanding suggests that dark matter does not decay like ordinary particles, ongoing research may reveal more about this elusive component of the universe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "siDS3KRVvZNv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}